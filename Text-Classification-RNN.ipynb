{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea40c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffdaa66",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2edafab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.curdir, \"data\")\n",
    "vocab_path = os.path.join(data_dir, \"word-level-vocab.json\")\n",
    "dataset_path = \"https://nlp-slides.vercel.app/clean-tweets.tsv\"\n",
    "\n",
    "with open(vocab_path, \"rt\") as f:\n",
    "    vocab = json.load(f)\n",
    "    \n",
    "dataset = pd.read_csv(filepath_or_buffer=dataset_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "975f8462",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = dataset[\"clean_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189bb3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 961\n"
     ]
    }
   ],
   "source": [
    "OOV_TOKEN = \"[OOV]\"\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "\n",
    "OOV_INDEX = vocab.get(OOV_TOKEN)\n",
    "\n",
    "print(f\"Vocab Size = {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79f4714",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = [[vocab.get(token) for token in tweet.split(\" \") if token in vocab] for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b066def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32002847",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2520e8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tweets that are empty (all tokens appear less than the minimum frequency)\n",
    "len([len(tweet) for tweet in tokenized_tweets if len(tweet) == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b1fdd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max Tweet length\n",
    "max([len(tweet) for tweet in tokenized_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5996106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 99th percentile of tweets' lengths\n",
    "np.percentile([len(tweet) for tweet in  tokenized_tweets], q=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e8c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered Tweets indices by length\n",
    "MIN_LENGTH = 0\n",
    "MAX_LENGTH = np.percentile([len(tweet) for tweet in  tokenized_tweets], q=99)\n",
    "filtered_indices = [index for index, tweet in enumerate(tokenized_tweets) if len(tweet) < MAX_LENGTH and len(tweet) > MIN_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef7ba7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nn.utils.rnn.pad_sequence([torch.tensor(tweet) for tweet in tokenized_tweets],\n",
    "                              batch_first=True,\n",
    "                              padding_value=vocab.get(PAD_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baa3eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(dataset[\"Sentiment\"].replace({\"negative\": 0, \"neutral\": 1, \"positive\": 2}).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0102eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X[filtered_indices], y[filtered_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79883725",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be1a28d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66a3462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa430670",
   "metadata": {},
   "source": [
    "# Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58d518dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.Word2Vec(sentences=tokenized_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8baa6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding.from_pretrained(embeddings=torch.tensor(word2vec.wv.vectors)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df43ded5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(959, 100)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7facdb1",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b79bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_layer: nn.Embedding, latent_dim: int, padding_idx: int = 0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = embedding_layer\n",
    "        self.embedding_dim = embedding_layer.embedding_dim\n",
    "#         self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "#         self.embedding = nn.Embedding(num_embeddings=self.vocab_size, \n",
    "#                                       embedding_dim=self.embedding_dim, \n",
    "#                                       padding_idx=self.padding_idx, \n",
    "#                                       max_norm=1.0)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.latent_dim, \n",
    "                          batch_first=True, num_layers=2, bidirectional=True)\n",
    "        \n",
    "       \n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=self.latent_dim * 2, out_features=self.latent_dim)\n",
    "        self.fc2 = nn.Linear(in_features=self.latent_dim, out_features=3)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = (batch_size, sequence_length) \n",
    "        \n",
    "        embeddings = embedding_layer(x)       \n",
    "        \n",
    "        # embeddings = (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        hidden_states, (last_hidden_state, last_cell_state) = self.rnn(embeddings)\n",
    "        \n",
    "        output = torch.concat([last_hidden_state[-1,:,:], last_hidden_state[-2,:,:]], dim=-1) \n",
    "        \n",
    "        output = torch.relu(self.fc1(output))\n",
    "        \n",
    "        output = self.fc2(output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return output\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52980f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "LATENT_DIM = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46c9ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentAnalyzer(vocab_size=len(vocab), embedding_layer=embedding_layer, latent_dim=LATENT_DIM).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d13fafb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "762f0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "optim = torch.optim.Adam(params=model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_metric = torchmetrics.F1Score(num_classes=3).to(device)\n",
    "val_metric = torchmetrics.F1Score(num_classes=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2475afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataclass object defined to store the loss and metric results of each epoch\n",
    "# REFERENCE: https://realpython.com/python-data-classes/\n",
    "@dataclass\n",
    "class Epoch:\n",
    "    epoch: int\n",
    "    training_loss: float\n",
    "    validation_loss: float\n",
    "    training_acc: float\n",
    "    validation_acc: float\n",
    "        \n",
    "    \n",
    "    def log(self) -> None:\n",
    "        print(f\"Epoch {self.epoch + 1}: Training Loss: {self.training_loss}\\tValidation Loss: {self.validation_loss} || Training F1: {self.training_acc}\\tValidation F1: {self.validation_acc}\\n----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd0d5918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [67,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [66,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m inputs, target \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# TODO: get prediction from model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# TODO: Calculate loss and metric\u001b[39;00m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, target)\n",
      "File \u001b[0;32m~/hdd/shai-courses/nlp-course-notebooks/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36mSentimentAnalyzer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding_layer(x)       \n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# embeddings = (batch_size, sequence_length, embedding_dim)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m hidden_states, (last_hidden_state, last_cell_state) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([last_hidden_state[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:,:], last_hidden_state[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,:,:]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m     37\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(output))\n",
      "File \u001b[0;32m~/hdd/shai-courses/nlp-course-notebooks/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hdd/shai-courses/nlp-course-notebooks/venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    773\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "history: list[Epoch] = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # TODO: Define epoch train and test: loss and metrics\n",
    "    # TODO: Make sure to reset the metric function\n",
    "    epoch_train_loss = 0\n",
    "    epoch_val_loss = 0\n",
    "    \n",
    "    train_metric.reset()\n",
    "    val_metric.reset()\n",
    "    \n",
    "    # TODO: Set the model to training mode\n",
    "    model.train()\n",
    "    # Iterate train batches\n",
    "    for batch in train_dataloader:\n",
    "        \n",
    "        # REMEMBER: you need to reset the optimizer in order to avoid incorrect accumulation of gradientrs \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # TODO: Unpack the batch and move it to the correct device\n",
    "        inputs, target = batch\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "\n",
    "        # TODO: get prediction from model\n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        # TODO: Calculate loss and metric\n",
    "        \n",
    "        loss = criterion(predictions, target)\n",
    "        train_metric(predictions, target)\n",
    "        \n",
    "        # TODO: Backward propagataion\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        # Accumulate train loss\n",
    "        epoch_train_loss += loss.item()\n",
    "    \n",
    "    # Calculate epoch training metric\n",
    "    epoch_train_f1 = round(train_metric.compute().item(), 3)\n",
    "\n",
    "    \n",
    "    # TODO: set the model to test mode\n",
    "    model.eval()\n",
    "    # Reset the metric function\n",
    "    # Make sure PyTorch will run inference without tracking gradients for enhancing performance\n",
    "    with torch.no_grad():\n",
    "        # TODO: Load batches from `test_loader`\n",
    "        for batch in test_dataloader:\n",
    "            \n",
    "            # TODO: Unpack the batch and move it to the correct device\n",
    "            inputs, target = batch\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "\n",
    "            # TODO: get prediction from model\n",
    "            predictions = model(inputs)\n",
    "            # TODO: Calculate loss and metric values\n",
    "            loss = criterion(predictions, target)\n",
    "            val_metric(predictions, target)\n",
    "            \n",
    "            # TODO: Accumulate validation loss\n",
    "            epoch_val_loss += loss.item()\n",
    "            \n",
    "        # TODO: Calculate epoch validation metric\n",
    "        epoch_val_f1 = round(val_metric.compute().item(), 3)\n",
    "      \n",
    "\n",
    "    # Calculate epoch training loss\n",
    "    epoch_train_loss = round(epoch_train_loss / len(train_dataloader), 3)\n",
    "    # TODO: Calculate epoch validation loss\n",
    "    epoch_val_loss = round(epoch_val_loss / len(test_dataloader), 3)\n",
    "    \n",
    "    # TODO: Create `Epoch` instance with the results\n",
    "    epoch_result = Epoch(epoch=epoch, \n",
    "                         training_loss=epoch_train_loss, \n",
    "                         validation_loss=epoch_val_loss,\n",
    "                         training_acc=epoch_train_f1,\n",
    "                         validation_acc=epoch_val_f1,\n",
    "                        )\n",
    "    \n",
    "    # Add to history list\n",
    "    history.append(epoch_result)\n",
    "    # Log epoch output\n",
    "    epoch_result.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = list(map(lambda x: x.training_loss ,history))\n",
    "val_loss = list(map(lambda x: x.validation_loss ,history))\n",
    "train_acc = list(map(lambda x: x.training_acc, history))\n",
    "val_acc = list(map(lambda x: x.validation_acc, history))\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(15, 12), ncols=2)\n",
    "\n",
    "ax1.plot(range(EPOCHS), train_loss, label=\"Training\")\n",
    "ax1.plot(range(EPOCHS), val_loss, label=\"Validation\")\n",
    "\n",
    "ax2.plot(range(EPOCHS), train_acc, label=\"Training\")\n",
    "ax2.plot(range(EPOCHS), val_acc, label=\"Validation\")\n",
    "\n",
    "ax1.set_title(\"Loss\", fontdict=dict(size=15), pad=15)\n",
    "ax2.set_title(\"F1\", fontdict=dict(size=15), pad=15)\n",
    "\n",
    "ax1.set_xticks(range(EPOCHS))\n",
    "ax2.set_xticks(range(EPOCHS))\n",
    "\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
